{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lab 3: Model Selection in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Global imports and settings\n",
    "from preamble import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluation procedures\n",
    "### Holdout\n",
    "The simplest procedure is [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html), which splits arrays or matrices into random train and test subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set score: 0.92\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# create a synthetic dataset\n",
    "X, y = make_blobs(centers=2, random_state=0)\n",
    "# split data and labels into a training and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "# Instantiate a model and fit it to the training set\n",
    "model = LogisticRegression().fit(X_train, y_train)\n",
    "# evaluate the model on the test set\n",
    "print(\"Test set score: {:.2f}\".format(model.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cross-validation\n",
    "- [cross_val_score](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html?highlight=cross%20val%20score#sklearn.model_selection.cross_val_score)\n",
    "    - `cv` parameter defines the kind of cross-validation splits, default is 5-fold CV\n",
    "    - `scoring` defines the scoring metric. Also see below.\n",
    "    - Returns list of all scores. Models are built internally, but not returned\n",
    "- [cross_validate](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html?highlight=cross%20validate#sklearn.model_selection.cross_validate)\n",
    "    - Similar, but also returns the fit and test times, and allows multiple scoring metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.967 1.    0.933 0.967 1.   ]\n",
      "Average cross-validation score: 0.97\n",
      "Variance in cross-validation score: 0.0006\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "iris = load_iris()\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "scores = cross_val_score(logreg, iris.data, iris.target, cv=5)\n",
    "print(\"Cross-validation scores: {}\".format(scores))\n",
    "print(\"Average cross-validation score: {:.2f}\".format(scores.mean()))\n",
    "print(\"Variance in cross-validation score: {:.4f}\".format(np.var(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Custom CV splits\n",
    "- You can build folds manually with [KFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html?highlight=kfold#sklearn.model_selection.KFold) or [StratifiedKFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold)\n",
    "    - randomizable (`shuffle` parameter)\n",
    "- [LeaveOneOut](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneOut.html?highlight=leave%20one%20out#sklearn.model_selection.LeaveOneOut) does leave-one-out cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores KFold(n_splits=5):\n",
      "[1.    1.    0.867 0.933 0.833]\n",
      "Cross-validation scores StratifiedKFold(n_splits=5, shuffle=True):\n",
      "[0.967 0.933 0.967 0.967 0.967]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "kfold = KFold(n_splits=5)\n",
    "print(\"Cross-validation scores KFold(n_splits=5):\\n{}\".format(\n",
    "      cross_val_score(logreg, iris.data, iris.target, cv=kfold)))\n",
    "skfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "print(\"Cross-validation scores StratifiedKFold(n_splits=5, shuffle=True):\\n{}\".format(\n",
    "      cross_val_score(logreg, iris.data, iris.target, cv=skfold)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cv iterations:  150\n",
      "Mean accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "loo = LeaveOneOut()\n",
    "scores = cross_val_score(logreg, iris.data, iris.target, cv=loo)\n",
    "print(\"Number of cv iterations: \", len(scores))\n",
    "print(\"Mean accuracy: {:.2f}\".format(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Shuffle-split\n",
    "These shuffle the data before splitting it.\n",
    "- `ShuffleSplit` and `StratifiedShuffleSplit` (recommended for classification)\n",
    "- `train_size` and `test_size` can be absolute numbers or a percentage of the total dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores:\n",
      "[0.973 0.973 0.973 0.96  0.973 0.973 0.947 0.933 0.973 0.987]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit, StratifiedShuffleSplit\n",
    "shuffle_split = StratifiedShuffleSplit(test_size=.5, train_size=.5, n_splits=10)\n",
    "scores = cross_val_score(logreg, iris.data, iris.target, cv=shuffle_split)\n",
    "print(\"Cross-validation scores:\\n{}\".format(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Grouped cross-validation\n",
    "- Add an array with group membership to `cross_val_scores` \n",
    "- Use `GroupKFold` with the number of groups as CV procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross_val_score(logreg, X, y, groups, cv=GroupKFold(n_splits=4)\n",
      "Cross-validation scores :\n",
      "[0.667 0.667 1.    0.667]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "# create synthetic dataset\n",
    "X, y = make_blobs(n_samples=12, random_state=0)\n",
    "# the first three samples belong to the same group, etc.\n",
    "groups = [0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3]\n",
    "scores = cross_val_score(logreg, X, y, groups, cv=GroupKFold(n_splits=4))\n",
    "print(\"cross_val_score(logreg, X, y, groups, cv=GroupKFold(n_splits=4)\")\n",
    "print(\"Cross-validation scores :\\n{}\".format(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Binary classification\n",
    "- [confusion_matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html?highlight=confusion%20matrix#sklearn.metrics.confusion_matrix) returns a matrix counting how many test examples are predicted correctly or 'confused' with other metrics.\n",
    "- [sklearn.metrics](https://scikit-learn.org/stable/modules/classes.html?highlight=metrics#module-sklearn.metrics) contains implementations many of the metrics discussed in class\n",
    "    - They are all implemented so that 'higher is better'. \n",
    "- [accuracy_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score) computes accuracy explictly\n",
    "- [classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) returns a table of binary measures, per class, and aggregated according to different aggregation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion_matrix(y_test, y_pred): \n",
      " [[48  5]\n",
      " [ 5 85]]\n",
      "accuracy_score(y_test, y_pred):  0.9300699300699301\n",
      "model.score(X_test, y_test):  0.9300699300699301\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "data = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.data, data.target, stratify=data.target, random_state=0)\n",
    "lr = LogisticRegression().fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "print(\"confusion_matrix(y_test, y_pred): \\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"accuracy_score(y_test, y_pred): \", accuracy_score(y_test, y_pred))\n",
    "print(\"model.score(X_test, y_test): \", lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.91      0.91        53\n",
      "           1       0.94      0.94      0.94        90\n",
      "\n",
      "    accuracy                           0.93       143\n",
      "   macro avg       0.93      0.93      0.93       143\n",
      "weighted avg       0.93      0.93      0.93       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "plt.rcParams['figure.dpi'] = 100 \n",
    "print(classification_report(y_test, lr.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can explictly define the averaging function for class-level metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro average f1 score: 0.930\n",
      "Weighted average f1 score: 0.930\n",
      "Macro average f1 score: 0.925\n"
     ]
    }
   ],
   "source": [
    "pred = lr.predict(X_test)\n",
    "print(\"Micro average f1 score: {:.3f}\".format(f1_score(y_test, pred, average=\"micro\")))\n",
    "print(\"Weighted average f1 score: {:.3f}\".format(f1_score(y_test, pred, average=\"weighted\")))\n",
    "print(\"Macro average f1 score: {:.3f}\".format(f1_score(y_test, pred, average=\"macro\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probabilistic predictions\n",
    "To retrieve the uncertainty in the prediction, scikit-learn offers 2 functions. Often, both are available for every learner, but not always.\n",
    "\n",
    "- decision_function: returns floating point (-Inf,Inf) value for each prediction\n",
    "- predict_proba: returns probability [0,1] for each prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can also use these to compute any metric with non-standard thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold -0.8\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.89      0.91        53\n",
      "           1       0.94      0.97      0.95        90\n",
      "\n",
      "    accuracy                           0.94       143\n",
      "   macro avg       0.94      0.93      0.93       143\n",
      "weighted avg       0.94      0.94      0.94       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Threshold -0.8\")\n",
    "y_pred_lower_threshold = lr.decision_function(X_test) > -.8\n",
    "print(classification_report(y_test, y_pred_lower_threshold))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Precision-Recall and ROC curves\n",
    "\n",
    "- [precision_recall_curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html?highlight=precision_recall_curve) returns all precision and recall values for all possible thresholds\n",
    "- [roc_curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html?highlight=roc%20curve#sklearn.metrics.roc_curve) does the same for TPR and FPR.\n",
    "- The average precision score is returned by the `average_precision_score` measure \n",
    "- The area under the ROC curve is returned by the `roc_auc_score` measure \n",
    "    - Don't use `auc` (this uses a less accurate trapezoidal rule)\n",
    "    - Require a decision function or predict_proba.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "precision, recall, thresholds = precision_recall_curve(\n",
    "    y_test, lr.decision_function(X_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision of logreg: 0.996\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "ap_pp = average_precision_score(y_test, lr.predict_proba(X_test)[:, 1])\n",
    "ap_df = average_precision_score(y_test, lr.decision_function(X_test))\n",
    "print(\"Average precision of logreg: {:.3f}\".format(ap_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC for Random Forest: 0.992\n",
      "AUC for SVC: 0.992\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "rf_auc = roc_auc_score(y_test, lr.predict_proba(X_test)[:, 1])\n",
    "svc_auc = roc_auc_score(y_test, lr.decision_function(X_test))\n",
    "print(\"AUC for Random Forest: {:.3f}\".format(rf_auc))\n",
    "print(\"AUC for SVC: {:.3f}\".format(svc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro average f1 score: 0.930\n",
      "Weighted average f1 score: 0.930\n",
      "Macro average f1 score: 0.925\n"
     ]
    }
   ],
   "source": [
    "print(\"Micro average f1 score: {:.3f}\".format(f1_score(y_test, pred, average=\"micro\")))\n",
    "print(\"Weighted average f1 score: {:.3f}\".format(f1_score(y_test, pred, average=\"weighted\")))\n",
    "print(\"Macro average f1 score: {:.3f}\".format(f1_score(y_test, pred, average=\"macro\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Using evaluation metrics in model selection\n",
    "\n",
    "- You typically want to use AUC or other relevant measures in `cross_val_score` and `GridSearchCV` instead of the default accuracy.\n",
    "- scikit-learn makes this easy through the `scoring` argument\n",
    "    - But, you need to need to look the [mapping between the scorer and the metric](http://scikit-learn.org/stable/modules/model_evaluation.html#model-evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![scorers](../images/03_scoring.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Or simply look up like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available scorers:\n",
      "['accuracy', 'adjusted_mutual_info_score', 'adjusted_rand_score', 'average_precision', 'balanced_accuracy', 'completeness_score', 'explained_variance', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'fowlkes_mallows_score', 'homogeneity_score', 'jaccard', 'jaccard_macro', 'jaccard_micro', 'jaccard_samples', 'jaccard_weighted', 'max_error', 'mutual_info_score', 'neg_brier_score', 'neg_log_loss', 'neg_mean_absolute_error', 'neg_mean_gamma_deviance', 'neg_mean_poisson_deviance', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_median_absolute_error', 'neg_root_mean_squared_error', 'normalized_mutual_info_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc', 'roc_auc_ovo', 'roc_auc_ovo_weighted', 'roc_auc_ovr', 'roc_auc_ovr_weighted', 'v_measure_score']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.scorer import SCORERS\n",
    "print(\"Available scorers:\\n{}\".format(sorted(SCORERS.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Cross-validation with AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default scoring: [0.975 0.992 1.    0.994 0.981]\n",
      "Explicit accuracy scoring: [0.975 0.992 1.    0.994 0.981]\n",
      "AUC scoring: [0.997 0.999 1.    1.    0.984]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn .svm import SVC\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "\n",
    "# default scoring for classification is accuracy\n",
    "print(\"Default scoring: {}\".format(\n",
    "      cross_val_score(SVC(), digits.data, digits.target == 9)))\n",
    "# providing scoring=\"accuracy\" doesn't change the results\n",
    "explicit_accuracy =  cross_val_score(SVC(), digits.data, digits.target == 9, \n",
    "                                     scoring=\"accuracy\")\n",
    "print(\"Explicit accuracy scoring: {}\".format(explicit_accuracy))\n",
    "roc_auc =  cross_val_score(SVC(), digits.data, digits.target == 9,\n",
    "                           scoring=\"roc_auc\")\n",
    "print(\"AUC scoring: {}\".format(roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hyperparameter tuning\n",
    "Now that we know how to evaluate models, we can improve them by tuning their hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Grid search\n",
    "- Create a parameter grid as a dictionary\n",
    "    - Keys are parameter names\n",
    "    - Values are lists of hyperparameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter grid:\n",
      "{'C': [0.001, 0.01, 0.1, 1, 10, 100], 'gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "              'gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "print(\"Parameter grid:\\n{}\".format(param_grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- `GridSearchCV`: like a classifier that uses CV to automatically optimize its hyperparameters internally\n",
    "    - Input: (untrained) model, parameter grid, CV procedure\n",
    "    - Output: optimized model on given training data\n",
    "    - Should only have access to training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=SVC(C=1.0, break_ties=False, cache_size=200,\n",
       "                           class_weight=None, coef0=0.0,\n",
       "                           decision_function_shape='ovr', degree=3,\n",
       "                           gamma='scale', kernel='rbf', max_iter=-1,\n",
       "                           probability=False, random_state=None, shrinking=True,\n",
       "                           tol=0.001, verbose=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
       "                         'gamma': [0.001, 0.01, 0.1, 1, 10, 100]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.svm import SVC\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        iris.data, iris.target, random_state=0)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The optimized test score and hyperparameters can easily be retrieved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set score: 0.97\n"
     ]
    }
   ],
   "source": [
    "print(\"Test set score: {:.2f}\".format(grid_search.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 10, 'gamma': 0.1}\n",
      "Best cross-validation score: 0.97\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator:\n",
      "SVC(C=10, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "print(\"Best estimator:\\n{}\".format(grid_search.best_estimator_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When hyperparameters depend on other parameters, we can use lists of dictionaries to define the hyperparameter space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of grids:\n",
      "[{'kernel': ['rbf'], 'C': [0.001, 0.01, 0.1, 1, 10, 100], 'gamma': [0.001, 0.01, 0.1, 1, 10, 100]}, {'kernel': ['linear'], 'C': [0.001, 0.01, 0.1, 1, 10, 100]}]\n"
     ]
    }
   ],
   "source": [
    "param_grid = [{'kernel': ['rbf'],\n",
    "               'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "               'gamma': [0.001, 0.01, 0.1, 1, 10, 100]},\n",
    "              {'kernel': ['linear'],\n",
    "               'C': [0.001, 0.01, 0.1, 1, 10, 100]}]\n",
    "print(\"List of grids:\\n{}\".format(param_grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Nested cross-validation\n",
    "\n",
    "- Nested cross-validation:\n",
    "    - Outer loop: split data in training and test sets\n",
    "    - Inner loop: run grid search, splitting the training data into train and validation sets\n",
    "- Result is a just a list of scores\n",
    "    - There will be multiple optimized models and hyperparameter settings (not returned)\n",
    "- To apply on future data, we need to train `GridSearchCV` on all data again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores:  [0.967 1.    0.9   0.967 1.   ]\n",
      "Mean cross-validation score:  0.9666666666666668\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(GridSearchCV(SVC(), param_grid, cv=5),\n",
    "                         iris.data, iris.target, cv=5)\n",
    "print(\"Cross-validation scores: \", scores)\n",
    "print(\"Mean cross-validation score: \", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Parallelizing cross-validation and grid-search\n",
    "- On a practical note, it is easy to parallellize CV and grid search\n",
    "- `cross_val_score` and `GridSearchCV` have a `n_jobs` parameter defining the number of cores it can use.\n",
    "    - set it to `n_jobs=-1` to use all available cores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Random Search\n",
    "- `RandomizedSearchCV` works like `GridSearchCV`\n",
    "- Has `n_iter` parameter for the number of iterations\n",
    "- Search grid can use distributions instead of fixed lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=None, error_score=nan,\n",
       "                   estimator=SVC(C=1.0, break_ties=False, cache_size=200,\n",
       "                                 class_weight=None, coef0=0.0,\n",
       "                                 decision_function_shape='ovr', degree=3,\n",
       "                                 gamma='scale', kernel='rbf', max_iter=-1,\n",
       "                                 probability=False, random_state=None,\n",
       "                                 shrinking=True, tol=0.001, verbose=False),\n",
       "                   iid='deprecated', n_iter=20, n_jobs=None,\n",
       "                   param_distributions={'C': <scipy.stats._distn_infrastructure.rv_frozen object at 0x11d558828>,\n",
       "                                        'gamma': <scipy.stats._distn_infrastructure.rv_frozen object at 0x11d558eb8>},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import expon\n",
    "\n",
    "param_grid = {'C': expon(scale=100), \n",
    "              'gamma': expon(scale=.1)}\n",
    "random_search = RandomizedSearchCV(SVC(), param_distributions=param_grid,\n",
    "                                   n_iter=20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        iris.data, iris.target, random_state=0)\n",
    "random_search.fit(X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
