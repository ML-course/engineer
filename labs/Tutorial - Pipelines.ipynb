{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lab 4: Data engineering pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Global imports and settings\n",
    "from preamble import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Building Pipelines\n",
    "* In scikit-learn, a `pipeline` combines multiple processing _steps_ in a single estimator\n",
    "* All but the last step should be transformer (have a `transform` method)\n",
    "    * The last step can be a transformer too (e.g. Scaler+PCA)\n",
    "* It has a `fit`, `predict`, and `score` method, just like any other learning algorithm\n",
    "* Pipelines are built as a list of steps, which are (name, algorithm) tuples\n",
    "    * The name can be anything you want, but can't contain `'__'`\n",
    "    * We use `'__'` to refer to the hyperparameters, e.g. `svm__C`\n",
    "* Let's build, train, and score a `MinMaxScaler` + `LinearSVC` pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "``` python\n",
    "pipe = Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", LinearSVC())])\n",
    "pipe.fit(X_train, y_train).score(X_test, y_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.97\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "pipe = Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", LinearSVC())])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target,\n",
    "                                                    random_state=1)\n",
    "pipe.fit(X_train, y_train)\n",
    "print(\"Test score: {:.2f}\".format(pipe.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "* Now with cross-validation:\n",
    "``` python\n",
    "scores = cross_val_score(pipe, cancer.data, cancer.target)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.982 0.974 0.965 0.965 0.991]\n",
      "Average cross-validation score: 0.98\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(pipe, cancer.data, cancer.target)\n",
    "print(\"Cross-validation scores: {}\".format(scores))\n",
    "print(\"Average cross-validation score: {:.2f}\".format(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* We can retrieve the trained SVM by querying the right step indices\n",
    "``` python\n",
    "pipe.steps[1][1]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM component: LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "          verbose=0)\n"
     ]
    }
   ],
   "source": [
    "pipe.fit(X_train, y_train)\n",
    "print(\"SVM component: {}\".format(pipe.steps[1][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "* Or we can use the `named_steps` dictionary\n",
    "``` python\n",
    "pipe.named_steps['svm']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM component: LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "          verbose=0)\n"
     ]
    }
   ],
   "source": [
    "print(\"SVM component: {}\".format(pipe.named_steps['svm']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* When you don't need specific names for specific steps, you can use `make_pipeline`\n",
    "    * Assigns names to steps automatically\n",
    "``` python\n",
    "pipe_short = make_pipeline(MinMaxScaler(), LinearSVC(C=100))\n",
    "print(\"Pipeline steps:\\n{}\".format(pipe_short.steps))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline steps:\n",
      "[('minmaxscaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('linearsvc', LinearSVC(C=100, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "          verbose=0))]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "# abbreviated syntax\n",
    "pipe_short = make_pipeline(MinMaxScaler(), LinearSVC(C=100))\n",
    "print(\"Pipeline steps:\\n{}\".format(pipe_short.steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Visualization of a pipeline `fit` and `predict`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/07_pipelines.png\" alt=\"ml\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Using Pipelines in Grid-searches\n",
    "* We can use the pipeline as a single estimator in `cross_val_score` or `GridSearchCV`\n",
    "* To define a grid, refer to the hyperparameters of the steps\n",
    "    * Step `svm`, parameter `C` becomes `svm__C`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'svm__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "              'svm__gamma': [0.001, 0.01, 0.1, 1, 10, 100]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation accuracy: 0.97\n",
      "Test set score: 0.97\n",
      "Best parameters: {'svm__C': 10, 'svm__gamma': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn import pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipe = pipeline.Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", SVC(C=100))])\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best cross-validation accuracy: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Test set score: {:.2f}\".format(grid.score(X_test, y_test)))\n",
    "print(\"Best parameters: {}\".format(grid.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* When we request the best estimator of the grid search, we'll get the best pipeline\n",
    "``` python\n",
    "grid.best_estimator_\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator:\n",
      "Pipeline(memory=None,\n",
      "         steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))),\n",
      "                ('svm',\n",
      "                 SVC(C=10, break_ties=False, cache_size=200, class_weight=None,\n",
      "                     coef0=0.0, decision_function_shape='ovr', degree=3,\n",
      "                     gamma=1, kernel='rbf', max_iter=-1, probability=False,\n",
      "                     random_state=None, shrinking=True, tol=0.001,\n",
      "                     verbose=False))],\n",
      "         verbose=False)\n"
     ]
    }
   ],
   "source": [
    "print(\"Best estimator:\\n{}\".format(grid.best_estimator_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "* And we can drill down to individual components and their properties\n",
    "``` python\n",
    "grid.best_estimator_.named_steps[\"svm\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM step:\n",
      "SVC(C=10, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma=1, kernel='rbf', max_iter=-1,\n",
      "    probability=False, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False)\n"
     ]
    }
   ],
   "source": [
    "# Get the SVM\n",
    "print(\"SVM step:\\n{}\".format(\n",
    "      grid.best_estimator_.named_steps[\"svm\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM support vector coefficients:\n",
      "[[ -1.392  -4.069  -0.435  -0.7    -5.865  -0.414  -2.814 -10.    -10.\n",
      "   -3.418  -7.908  -0.169  -4.299  -1.137  -2.214  -0.19  -10.     -7.128\n",
      "  -10.     -0.522  -3.766  -0.012  -1.159 -10.     -0.513  -0.712 -10.\n",
      "   -1.501 -10.     10.      1.995   0.909   0.919   2.897   0.399  10.\n",
      "    9.811   0.412  10.     10.     10.      5.415   0.83    2.593   1.371\n",
      "   10.      0.279   1.555   6.589   1.487  10.      1.156   0.391   2.663\n",
      "    1.277   0.651   1.841   2.395   2.504]]\n"
     ]
    }
   ],
   "source": [
    "# Get the SVM dual coefficients (support vector weights)\n",
    "print(\"SVM support vector coefficients:\\n{}\".format(\n",
    "      grid.best_estimator_.named_steps[\"svm\"].dual_coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Grid-searching preprocessing steps and model parameters\n",
    "* We can use grid search to optimize the hyperparameters of our preprocessing steps and learning algorithms at the same time\n",
    "* Consider the following pipeline:\n",
    "    - `StandardScaler`, without hyperparameters\n",
    "    - `PolynomialFeatures`, with the max. _degree_ of polynomials\n",
    "    - `Ridge` regression, with L2 regularization parameter _alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "boston = load_boston()\n",
    "X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target,\n",
    "                                                    random_state=0)\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "pipe = pipeline.make_pipeline(\n",
    "    StandardScaler(),\n",
    "    PolynomialFeatures(),\n",
    "    Ridge())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* We don't know the optimal polynomial degree or alpha value, so we use a grid search (or random search) to find the optimal values\n",
    "``` python\n",
    "param_grid = {'polynomialfeatures__degree': [1, 2, 3],\n",
    "              'ridge__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, cv=5, n_jobs=1)\n",
    "grid.fit(X_train, y_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'polynomialfeatures__degree': [1, 2, 3],\n",
    "              'ridge__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "# Note: I had to use n_jobs=1. (n_jobs=-1 stalls on my machine)\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, cv=5, n_jobs=1)\n",
    "grid.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Visualing the $R^2$ results as a heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.matshow(grid.cv_results_['mean_test_score'].reshape(3, -1),\n",
    "            vmin=0, cmap=\"viridis\")\n",
    "plt.xlabel(\"ridge__alpha\")\n",
    "plt.ylabel(\"polynomialfeatures__degree\")\n",
    "plt.xticks(range(len(param_grid['ridge__alpha'])), param_grid['ridge__alpha'])\n",
    "plt.yticks(range(len(param_grid['polynomialfeatures__degree'])),\n",
    "           param_grid['polynomialfeatures__degree'])\n",
    "\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Here, degree-2 polynomials help (but degree-3 ones don't), and tuning the alpha parameter helps as well.\n",
    "* Not using the polynomial features leads to suboptimal results (see the results for degree 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'polynomialfeatures__degree': 2, 'ridge__alpha': 10}\n",
      "Test-set score: 0.77\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters: {}\".format(grid.best_params_))\n",
    "print(\"Test-set score: {:.2f}\".format(grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FeatureUnions\n",
    "- Sometimes you want to apply multiple preprocessing techniques and use the _combined_ produced features\n",
    "- Simply appending the produced features is called a `FeatureJoin`\n",
    "- Example: Apply both PCA and feature selection, and run an SVM on both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined space has 3 features\n",
      "Pipeline(memory=None,\n",
      "         steps=[('features',\n",
      "                 FeatureUnion(n_jobs=None,\n",
      "                              transformer_list=[('pca',\n",
      "                                                 PCA(copy=True,\n",
      "                                                     iterated_power='auto',\n",
      "                                                     n_components=3,\n",
      "                                                     random_state=None,\n",
      "                                                     svd_solver='auto', tol=0.0,\n",
      "                                                     whiten=False)),\n",
      "                                                ('univ_select',\n",
      "                                                 SelectKBest(k=1,\n",
      "                                                             score_func=<function f_classif at 0x123dfe2f0>))],\n",
      "                              transformer_weights=None, verbose=False)),\n",
      "                ('svm',\n",
      "                 SVC(C=10, break_ties=False, cache_size=200, class_weight=None,\n",
      "                     coef0=0.0, decision_function_shape='ovr', degree=3,\n",
      "                     gamma='scale', kernel='linear', max_iter=-1,\n",
      "                     probability=False, random_state=None, shrinking=True,\n",
      "                     tol=0.001, verbose=False))],\n",
      "         verbose=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# This dataset is way too high-dimensional. Better do PCA:\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Maybe some original features where good, too?\n",
    "selection = SelectKBest(k=1)\n",
    "\n",
    "# Build estimator from PCA and Univariate selection:\n",
    "\n",
    "combined_features = FeatureUnion([(\"pca\", pca), (\"univ_select\", selection)])\n",
    "\n",
    "# Use combined features to transform dataset:\n",
    "X_features = combined_features.fit(X, y).transform(X)\n",
    "print(\"Combined space has\", X_features.shape[1], \"features\")\n",
    "\n",
    "svm = SVC(kernel=\"linear\")\n",
    "\n",
    "# Do grid search over k, n_components and C:\n",
    "\n",
    "pipeline = Pipeline([(\"features\", combined_features), (\"svm\", svm)])\n",
    "\n",
    "param_grid = dict(features__pca__n_components=[1, 2, 3],\n",
    "                  features__univ_select__k=[1, 2],\n",
    "                  svm__C=[0.1, 1, 10])\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid=param_grid)\n",
    "grid_search.fit(X, y)\n",
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ColumnTransformer\n",
    "- A pipeline applies a transformer on _all_ columns\n",
    "    - If your dataset has both numeric and categorical features, you often want to apply different techniques on each\n",
    "    - You _could_ manually split up the dataset, and then feature-join the processed features (tedious)\n",
    "- `ColumnTransformer` allows you to specify on which columns a preprocessor has to be run\n",
    "    - Either by specifying the feature names, indices, or a binary mask\n",
    "- You can include multiple transformers in a ColumnTransformer\n",
    "    - In the end the results will be feature-joined\n",
    "    - Hence, the order of the features will change!\n",
    "        The features of the last transformer will be at the end\n",
    "- Each transformer can be a pipeline\n",
    "    - Handy if you need to apply multiple preprocessing steps on a set of features\n",
    "    - E.g. use a ColumnTransformer with one sub-pipeline for numerical features and one for categorical features.\n",
    "- In the end, the columntransformer can again be included as part of a pipeline\n",
    "    - E.g. to add a classfier and include the whole pipeline in a grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Handle a dataset (Titanic) with both categorical an numeric features\n",
    "- Numeric features: impute missing values and scale\n",
    "- Categorical features: Impute missing values and apply one-hot-encoding\n",
    "- Finally, run an SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model score: 0.790\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Load data from https://www.openml.org/d/40945\n",
    "X, y = fetch_openml(\"titanic\", version=1, as_frame=True, return_X_y=True)\n",
    "\n",
    "# Alternatively X and y can be obtained directly from the frame attribute:\n",
    "# X = titanic.frame.drop('survived', axis=1)\n",
    "# y = titanic.frame['survived']\n",
    "\n",
    "# We will train our classifier with the following features:\n",
    "# Numeric Features:\n",
    "# - age: float.\n",
    "# - fare: float.\n",
    "# Categorical Features:\n",
    "# - embarked: categories encoded as strings {'C', 'S', 'Q'}.\n",
    "# - sex: categories encoded as strings {'female', 'male'}.\n",
    "# - pclass: ordinal integers {1, 2, 3}.\n",
    "\n",
    "# We create the preprocessing pipelines for both numeric and categorical data.\n",
    "numeric_features = ['age', 'fare']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_features = ['embarked', 'sex', 'pclass']\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', LogisticRegression())])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can again run optimize any of the hyperparameters (preprocessing-related ones included) in a grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best logistic regression from grid search: 0.798\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'preprocessor__num__imputer__strategy': ['mean', 'median'],\n",
    "    'classifier__C': [0.1, 1.0, 10, 100],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=10)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print((\"best logistic regression from grid search: %.3f\"\n",
    "       % grid_search.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
